---
author:
  - name: Ropafadzo Chimuti
    email: CHMROP002@myuct.ac.za
title: "Model Selection in Text Classification: A Study on Predictive Approaches for President Identification"
keywords: SONA, neural networks ,classification trees, convolutional neural networks ,bag-of-words,nn,cnn
abstract: |
  This report explores South African presidential State of the Nation Address (SONA) speeches. The aim is to construct predictive models that identify individual presidents by analyzing sentences from their SONA speeches. Text mining techniques are employed to analyse the semi-unstructured data.
  
  Six models were constructed using three predictive models: feedforwards neural network, convolutional neural network and classification tree, and two data structuring techniques were used: top words bag-of-words and converting text-to-sequence. The data was split into 70% training and 30% testing for all models. However, two of the neural networks constructed used 20% of the training data for cross-validation. The neural network models evaluated the text-to-sequence data, while the classification tree models evaluated the bag-of-words data.

  Results:
  Neural network models showed overfitting issues, while classification models, particularly the top 500 bag-of-words classification tree, demonstrated better consistency between training and testing data. Kappa statistics indicated a weak agreement between predicted and actual classifications.

  Conclusion:
  The top-performing model is a classification tree using the top 500 bag-of-words. To improve model performance, a larger and balanced sample of presidential speeches is needed.

editor: 
  markdown: 
    wrap: 72
---

```{r libraries, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
library(stringr)
library(tidyverse)
library(tidytext)
library(textstem)
library(textdata)
library(tm)
library(MASS)
library(topicmodels)
library(keras)
library(tensorflow)
library(readr)
library(lubridate)
library(tfhub)
library(dplyr)
library(rpart)
library(knitr)
library(caret)
library(smotefamily)
```

```{r extract, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
set.seed(2023)

# read in text data files and organise these into a data frame
filenames <- c('1994_post_elections_Mandela.txt', 
               '1994_pre_elections_deKlerk.txt', 
               '1995_Mandela.txt', 
               '1996_Mandela.txt', 
               '1997_Mandela.txt',
               '1998_Mandela.txt', 
               '1999_post_elections_Mandela.txt', 
               '1999_pre_elections_Mandela.txt', 
               '2000_Mbeki.txt', 
               '2001_Mbeki.txt', 
               '2002_Mbeki.txt', 
               '2003_Mbeki.txt', 
               '2004_post_elections_Mbeki.txt', 
               '2004_pre_elections_Mbeki.txt', 
               '2005_Mbeki.txt', 
               '2006_Mbeki.txt', 
               '2007_Mbeki.txt', 
               '2008_Mbeki.txt', 
               '2009_post_elections_Zuma.txt', 
               '2009_pre_elections_ Motlanthe.txt', 
               '2010_Zuma.txt', 
               '2011_Zuma.txt', 
               '2012_Zuma.txt', 
               '2013_Zuma.txt', 
               '2014_post_elections_Zuma.txt', 
               '2014_pre_elections_Zuma.txt', 
               '2015_Zuma.txt', 
               '2016_Zuma.txt', 
               '2017_Zuma.txt', 
               '2018_Ramaphosa.txt', 
               '2019_post_elections_Ramaphosa.txt', 
               '2019_pre_elections_Ramaphosa.txt', 
               '2020_Ramaphosa.txt', 
               '2021_Ramaphosa.txt', 
               '2022_Ramaphosa.txt', 
               '2023_Ramaphosa.txt')


this_speech <- c()
this_speech[1] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1994_post_elections_Mandela.txt', nchars = 27050)
this_speech[2] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1994_pre_elections_deKlerk.txt', nchars = 12786)
this_speech[3] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1995_Mandela.txt', nchars = 39019)
this_speech[4] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1996_Mandela.txt', nchars = 39524)
this_speech[5] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1997_Mandela.txt', nchars = 37489)
this_speech[6] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1998_Mandela.txt', nchars = 45247)
this_speech[7] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1999_post_elections_Mandela.txt', nchars = 34674)
this_speech[8] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1999_pre_elections_Mandela.txt', nchars = 41225)
this_speech[9] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2000_Mbeki.txt', nchars = 37552)
this_speech[10] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2001_Mbeki.txt', nchars = 41719)
this_speech[11] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2002_Mbeki.txt', nchars = 50544)
this_speech[12] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2003_Mbeki.txt', nchars = 58284)
this_speech[13] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2004_post_elections_Mbeki.txt', nchars = 34590)
this_speech[14] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2004_pre_elections_Mbeki.txt', nchars = 39232)
this_speech[15] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2005_Mbeki.txt', nchars = 54635)
this_speech[16] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2006_Mbeki.txt', nchars = 48643)
this_speech[17] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2007_Mbeki.txt', nchars = 48641)
this_speech[18] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2008_Mbeki.txt', nchars = 44907)
this_speech[19] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2009_post_elections_Zuma.txt', nchars = 31101)
this_speech[20] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2009_pre_elections_Motlanthe.txt', nchars = 47157)
this_speech[21] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2010_Zuma.txt', nchars = 26384)
this_speech[22] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2011_Zuma.txt', nchars = 33281)
this_speech[23] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2012_Zuma.txt', nchars = 33376)
this_speech[24] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2013_Zuma.txt', nchars = 36006)
this_speech[25] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2014_post_elections_Zuma.txt', nchars = 29403)
this_speech[26] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2014_pre_elections_Zuma.txt', nchars = 36233)
this_speech[27] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2015_Zuma.txt', nchars = 32860)
this_speech[28] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2016_Zuma.txt', nchars = 32464)
this_speech[29] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2017_Zuma.txt', nchars = 35981)
this_speech[30] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2018_Ramaphosa.txt', nchars = 33290)
this_speech[31] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2019_post_elections_Ramaphosa.txt', nchars = 42112)
this_speech[32] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2019_pre_elections_Ramaphosa.txt', nchars = 56960)
this_speech[33] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2020_Ramaphosa.txt', nchars = 47910)
this_speech[34] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2021_Ramaphosa.txt', nchars = 43352)
this_speech[35] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2022_Ramaphosa.txt', nchars = 52972)
this_speech[36] <- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2022_Ramaphosa.txt', nchars = 53933)


sona <- data.frame(filename = filenames, speech = this_speech, stringsAsFactors = FALSE)

# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$pres <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")

# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n%[0-9]'

sona <-sona %>%
  mutate(speech = lemmatize_strings(stringr::str_replace_all(speech, replace_reg , ' '))
         ,date = str_sub(speech, start=1, end=30)
         ,date = str_replace_all(date, "February", "02")
         ,date = str_replace_all(date, "June", "06")
         ,date = str_replace_all(date, "Feb", "02")
         ,date = str_replace_all(date, "May", "05")
         ,date = str_replace_all(date, "Jun", "06")
         ,date = str_replace_all(date, "Thursday, ","")
         ,date = str_replace_all(date, ' ', '-')        
         ,date = str_replace_all(date, "[A-z]",'')
         ,date = str_replace_all(date, '-----', '')
         ,date = str_replace_all(date, '----', '')
         ,date = str_replace_all(date, '---', '')
         ,date = str_replace_all(date, '--', '')
  )

# Convert to tibble
sona <- as.tibble(sona)

#tokenize into sentences 
tidy_sona <- sona %>% 
  mutate(speech = str_replace_all(speech, replace_reg, '')) %>% # remove links etx
  unnest_tokens(sentences, speech, token = 'sentences') %>%   # tokenize
  dplyr::select(sentences, pres, date, filename,year)

#tokenize into words 
tidy_sona.w <- sona %>% 
  mutate(speech = str_replace_all(speech, replace_reg, '')) %>% # remove links etx
  unnest_tokens(word, speech, token = 'regex') %>%   # tokenize
  filter(!word %in% stop_words$word, str_detect(word, '[A-Za-z]')) %>% #remove stop words
  dplyr::select(pres, date, word)

# removing deKlerk and Motlanthe from the analysis because they made only 1 speech
#sentences 
tidy_sona  <- tidy_sona  %>%
  filter(!pres %in% c('deKlerk', 'Motlanthe')) 
#words 
tidy_sona.w  <- tidy_sona.w  %>%
  filter(!pres %in% c('deKlerk', 'Motlanthe')) 

```

## Introduction

This report investigates speeches made by South African presidents at the State of the Nation Address (SONA). In these speeches the presidents address the nation on the prevailing situation, the SONA serves as an account of the country's progress, challenges, and policy directions. The analysis of these speeches over time can offer insights into the evolving political landscape, priorities of each presidency, and the utilization of language by different leaders.

This report aims to construct predictive models which can discern the distinctions between different presidents. These models will be designed to take a sentence from the SONA speeches as input and predict which President delivered it.

## literature Review

Text mining techniques were employed to achieve the research objectives. These techniques enable the analysis of unstructured textual data collections.

Neural Network: A neural network contains connected neurons with associated weights. It processes data chronologically and learns by comparing its classifications to the actual target variable \[[1](#1)\]. Neural networks possess robustness, self-learning capabilities, and adaptiveness, this makes them advanced classifiers \[[1](#1)\]. They consist of three essential layers: the input layer, the hidden/intermediate layer, and the output layer.

The neural network models in this study utilized specific activation functions:

Softmax Activation Function: This function is crucial in the output layer of neural networks, especially for multi-class classification. It converts output scores into class probabilities, ensuring that the probabilities sum to one.

Rectified Linear Unit (ReLU) Activation Function: ReLU replaces negative values with zero while retaining positive values. It is widely used in hidden layers of neural networks to expedite convergence during training and mitigate the vanishing gradient problem.

Classification Tree: Classification trees, also referred to as decision trees, are a data mining classification technique. They employ a branching method to represent decision outcomes \[[2](#2)\]. Decision trees learn from data to approximate decision-making processes through a set of IF-THEN rules \[[2](#2)\]. These trees are valuable for decision-making and have the ability to depict various possible outcomes \[[2](#2)\].

## Data and Methods

### Data Preprocessing

President speeches in text format form 1994-2023 were downloaded from the sona website and read into a data frame for preprocessing. The speeches were semi-structured but were of different sizes so tabulation could not be automated. Information like the name of the president who delivered the speech, the date and the year the speech was delivered were manual extracted from each speech for tabulation. The table below shows the number of speeches per president in the data set.

```{r speeches, echo=FALSE, eval=TRUE}
# number of speches per president
table(sona$pres)
```

It is evident from the table presented that the dataset is not balances. Over sampling or under sampling the dataset was contemplated, but because of the significant difference in sample sizes, removing the two underrepresented president speeches was opted for instead to avoid introducing biases to the data.

The way dates were formatted was adjusted to ensure consistency and unnecessary text like dollar signs and asterikes were also removed from the data to be analysed. Once the data was successfully tabulated in a satisfactory format, the speeches were tokenised into sentences and words for the analysis stage. The table below shows the number of sentences per president in the dataset.

```{r sentences, echo=FALSE, eval=TRUE}
# number of sentences per president
table(tidy_sona$pres)
```

### Text to Sequence

Neural networks were one of the methods implemented in this investigation to make the predictions. This method requires numeric inputs, so the data had to be transformed into numeric form. To do this, the dataset containing the sentences by each president were converted to sequence, with a maximum length of 70 and 1500 maximum features. Sentences of lengths less than 70 were padded with zeros to satisfy the length requirement.

The target variable was converted to a numeric categorical variable.The result of this transformation was target matrix of zeros and ones with four columns,each representing the respective presidents in the dataset.

### Bag of words

In this investigation, different data structuring techniques were employed to prepare the input data. A technique where a fixed number of the most frequently occurring words were selected from the dataset and assembled into a collection was used in this case. This collection of words are known as a "bag of words". The bag-of-words method is limited in that it loses important textual details, such as word arrangement, sentence organization, and contextual information. These are the very elements that humans rely on to comprehend and make sense of text.

This input data was used to fit the classification tree models.

### Testing and Training

Each input set was split into two datasets, one set to train the models and the other to test it. The training datasets contained 70% of the data and the testing sets contained the remaining 30%. The data was split in a balanced way to ensure that 70% of the data from each president was represented in the training set and the remaining 30% in the testing set.

```{r sent.datasplit, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
set.seed(2023)
#splitting sentences into testing and training  
sent_train<- tidy_sona %>%
  group_by("pres") %>%
  slice_sample(prop=0.7)%>%
  ungroup()%>%
  dplyr::select(pres,date,sentences)

sent_test <- tidy_sona %>%
  anti_join(sent_train)%>%
  dplyr::select(pres,date,sentences)

#separate features into y and x variables
x_train <- sent_train[3]
y_train <- sent_train[1]

x_test <- sent_test[3]
y_test <- sent_test[1]

####CONVERT TARGET INTO CAT ##########

# Create the mapping
mapping <- c("Ramaphosa" = 0, "Mandela" = 1, "Zuma" = 2, "Mbeki" = 3)

# Apply the mapping to the pres column and convert to categorical. 
y_train_cat <- y_train %>%
  mutate(pres = mapping[pres])%>%
  to_categorical()

y_test_cat <- y_test %>%
  mutate(pres = mapping[pres])%>%
  to_categorical()

# convert sentences to sequence 
sent.to.seq<- function(x,max_features,maxlen){
  # choose max_features most popular words
  tokenizer = text_tokenizer(num_words = max_features)
  fit_text_tokenizer(tokenizer,x$sentences)
  
  #convert text to sequence
  x.seq <- texts_to_sequences(tokenizer,x$sentences)

  # padding 
  x.seq <- x.seq %>% pad_sequences(maxlen = maxlen) 
  return(x.seq)
}

max_features <- 1500
maxlen <- 70
x_train.seq <-sent.to.seq(x_train,max_features,maxlen)
x_test.seq <- sent.to.seq(x_test,max_features,maxlen)
```

```{r top100, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
set.seed(2023)
# select the top 500 words 
word_bag <- tidy_sona.w %>%
  group_by(word) %>%
  count() %>%
  ungroup() %>%
  top_n(100, wt = n)
 
#join words with presidents 
sona.w_tdf <- tidy_sona.w %>%
  inner_join(word_bag) %>%
  group_by(pres,date,word) %>%
  count() %>%  
  group_by(pres) %>%
  mutate(total = sum(n)) %>%
  ungroup()

# show words in a wide format
bag_of_words <- sona.w_tdf %>% 
  dplyr::select(pres,date,word,n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0) 
  
#split testing and training 
set.seed(2023)
word_train <- bag_of_words %>% 
  group_by(pres) %>% 
  slice_sample(prop = 0.7) %>% 
  ungroup() 

word_test<- bag_of_words %>% 
  anti_join(word_train) 

#training data sets 
x_train.w<-word_train%>%
  dplyr::select(-c(date))
y_train.w<-word_train%>%
  dplyr::select(pres)
#testing data sets 
x_test.w<-word_test%>%
  dplyr::select(-c(date,pres))
y_test.w <-word_test%>%
  dplyr::select(pres)
```

### Methods

This analysis was conducted using R software,three different types of models were constructed to assess the pre-processed input data. Keras library functions were used to develop the neural network models presented in this study and Rpart library functions were used to fit the classification trees.

### Feedforward Neural Networks (FNN)

Feedforward neural network models were built to evaluate the text to sequence data. The models were designed for a multi-class classification with 4 units in the final dense layer. The dense layer used the softmax activation function to output a probability distribution over the classes. The model had an embedding layer as the input layer, this layer converts input sequences into a continuous vector space representation. The input layer was followed by a regularization layer with a dropout rate of 0.2, which means 20% of the neurons in the previous layer were randomly set to zero during each training epoch to helps mitigate overfitting. The dropout layer was followed by a hidden dense layer with 100 units and utilized ReLU activation. The model was trained with categorical cross-entropy loss and monitored with early stopping to optimize training.The adam optimiser with the learning rate set to 0.01 was used.

Two feedforward models with the same architecture were built to access the effect of cross validation and early stopping on accuracy. One model implemented cross validation and early stopping while the other did not.

```{r nn, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(2023)
nn_model <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
  layer_dropout(0.2) %>%
  layer_flatten() %>%
  layer_dense(100, activation = "relu") %>%
  layer_dense(4, activation = "softmax")


nn_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('accuracy'),
)

#impliment early stopping 
early_stopping <- callback_early_stopping(monitor = "val_loss", patience = 10)

nn_history <- nn_model %>% 
  fit(x_train.seq, y_train_cat, 
      epochs = 30, batch_size = 10, 
      validation_split = 0.2, shuffle = TRUE,
      callbacks = list(early_stopping), verbose = 0
  )

nn_results <- nn_model %>% evaluate(x_test.seq, y_test_cat, batch_size =5, verbose = 0)
fnn <- nn_model %>% predict(x_test.seq, batch_size = 5,verbose = 0)

```

```{r nn.cv, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(2023)
nn_history.cv <- nn_model %>% 
  fit(x_train.seq, y_train_cat, 
      epochs = 30, batch_size = 10, 
      shuffle = TRUE, verbose=0
  )

nn_results.cv <- nn_model %>% evaluate(x_test.seq, y_test_cat, batch_size =5, verbose = 0)
fnn.cv <- nn_model %>% predict(x_test.seq, batch_size = 5,verbose = 0)

```

```{r fig-a, message=FALSE, warning=FALSE, echo=FALSE,fig.cap=" FNN accuracy and loss of the training data set"}
set.seed(2023)
plot(nn_history.cv)+
  ggtitle("Forward Neural Networks Model Training Without Validation")+
  theme(plot.title = element_text(hjust = 0.5))
```

@fig-a shows a graph of the model training results when cross-validation and early stopping were not implemented.

```{r fig-b, message=FALSE, warning=FALSE, echo=FALSE,fig.cap="FNN accuracy and loss of the training and validation data sets with early stopping implemented "}
set.seed(2023)
plot(nn_history)+
  ggtitle("Forward Neural Networks Model Training and Cross-Validation")+
  theme(plot.title = element_text(hjust = 0.5))
```

@fig-b shows a graph of the model training results when cross-validation and early stopping were implemented.

Early stopping can only be implemented when there is cross-validation, in this case 20% of the training data was set aside for cross-validation at each epoch. The model was designed to stop training if the recorded validation loss continuously increased without improvement for 10 epochs. This was done to prevent overfitting to the training data and to select the optimal model that performs well on unseen data.

### Convolutional Neural Networks (CNN)

The convolutional neural network (cnn) models were designed to be very similar to the feedforward neural network models. The CNN models had a more complex architecture, involving one dimensional convolutional and max-pooling layers. The convolutional layer had 64 filters, a kernel size of 8, and it utilised the ReLU activation function. This layer is particularly effective for identifying specific patterns in text data. The max-pooling layer had a maximum pool size of 2. This layer is responsible for dimension reduction of the data, this makes the layers in the model more computationally efficient.

Like the previous section two CNN models were developed, one model implemented cross validation and early stopping while the other did not.

```{r cnn, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(2023)
cnn_model <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
  layer_dropout(0.2) %>%
  layer_conv_1d(filters = 64, kernel_size = 8, activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_flatten() %>%
  layer_dense(50, activation = "relu") %>%
  layer_dense(4, activation = "softmax")


cnn_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('accuracy'),
)
set.seed(2023)
cnn_history <- cnn_model %>% 
  fit(x_train.seq, y_train_cat, 
      epochs = 30, batch_size = 5, 
      validation_split = 0.2, shuffle = TRUE,
      callbacks = list(early_stopping),verbose = 0
  )

cnn_results <- cnn_model %>% evaluate(x_test.seq, y_test_cat, batch_size = 5, verbose = 0)
cnn <- cnn_model %>% predict(x_test.seq, batch_size = 5,verbose = 0)


```

```{r cnn.cv, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(2023)
cnn_history.cv <- cnn_model %>% 
  fit(x_train.seq, y_train_cat, 
      epochs = 30, batch_size = 5, 
      shuffle = TRUE,verbose = 0
  )

cnn_results.cv <- cnn_model %>% evaluate(x_test.seq, y_test_cat, batch_size = 5, verbose = 0)
cnn.cv <- cnn_model %>% predict(x_test.seq, batch_size = 5,verbose = 0)
```

```{r fig-c, message=FALSE, warning=FALSE, echo=FALSE,fig.cap=" CNN accuracy and loss of the training data set"}
set.seed(2023)
plot(cnn_history.cv)+
  ggtitle("Convolutional Neural Networks Model Training Without Validation")+
  theme(plot.title = element_text(hjust = 0.5))
```

@fig-c shows a graph of the model training results when cross-validation and early stopping were not implemented.

```{r fig-d, message=FALSE, warning=FALSE, echo=FALSE,fig.cap=" CNN accuracy and loss of the training and validation data sets with early stopping implemented"}
set.seed(2023)
plot(cnn_history)+
  ggtitle("Convolutional Neural Networks Model Training and Validation")+
  theme(plot.title = element_text(hjust = 0.5))
```

@fig-d shows a graph of the model training results when cross-validation and early stopping were implemented. 20% of the training data was set aside for validation at each epoch.

### Classification Trees

Two classification trees were fitted to the bag-of-words datasets. One bag-of-words dataset selected the top 100 words from the speeches while the other selected the top 500 words.

```{r 100 clas.t, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(2023)
#fit a classification tree
fit <- rpart(pres ~ .,x_train.w, method = 'class')

```

```{r 500 clas.t, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(2023)
# select the top 500 words 
word_bag <- tidy_sona.w %>%
  group_by(word) %>%
  count() %>%
  ungroup() %>%
  top_n(500, wt = n)
 
#join words with presidents 
sona.w_tdf <- tidy_sona.w %>%
  inner_join(word_bag) %>%
  group_by(pres,date,word) %>%
  count() %>%  
  group_by(pres) %>%
  mutate(total = sum(n)) %>%
  ungroup()

# show words in a wide format
bag_of_words <- sona.w_tdf %>% 
  dplyr::select(pres,date,word,n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0) 
  
#split testing and training 
set.seed(2023)
word_train <- bag_of_words %>% 
  group_by(pres) %>% 
  slice_sample(prop = 0.7) %>% 
  ungroup() 

word_test<- bag_of_words %>% 
  anti_join(word_train) 

#training data sets 
x_train.w<-word_train%>%
  dplyr::select(-c(date))
y_train.w<-word_train%>%
  dplyr::select(pres)
#testing data sets 
x_test.w<-word_test%>%
  dplyr::select(-c(date,pres))
y_test.w <-word_test%>%
  dplyr::select(pres)
#fit a classification tree
fit1 <- rpart(pres ~ .,x_train.w, method = 'class')

```

```{r fig-e, message=FALSE, warning=FALSE, echo=FALSE,fig.cap=" Classification trees for the top100 and top500 bag-of-words"}
set.seed(2023)
par(mfrow = c(1, 2)) 
plot(fit, main = 'Full Classification Tree Top100')
text(fit, use.n = TRUE, all = TRUE, cex=.5)
plot(fit1, main = 'Full Classification Tree Top500')
text(fit1, use.n = TRUE, all = TRUE, cex=.5)
```

@fig-e displays the classification trees generated using the bag-of-words input datasets. Notably, both resulting trees are classifying the input into just two categories, even though the task involves classifying into four distinct classes.

## Results and Discussion

Below is a table which shows the model accuracy results for the six models which were constructed.

```{r results clas.t, message=FALSE, warning=FALSE, echo=FALSE,out.width="80%"}
set.seed(2023)

#100
#predict using the training set 
fittedtrain <- predict(fit, type = 'class')
predtrain <- table(x_train.w$pres, fittedtrain)

# training accuracy
#round(sum(diag(predtrain))/sum(predtrain), 3) 

# fit model on test data 
fit.test<-predict(fit,newdata =x_test.w,type="class" )
predtrain.t <- table(y_test.w$pres, fit.test)


# testing accuracy
#round(sum(diag(predtrain.t))/sum(predtrain.t), 3) 


#predict using the training set 
fittedtrain1 <- predict(fit1, type = 'class')
predtrain1 <- table(x_train.w$pres, fittedtrain1)


# training accuracy
#round(sum(diag(predtrain1))/sum(predtrain1), 3) 

# fit model on test data 
fit.test1<-predict(fit1,newdata =x_test.w,type="class" )
predtrain.t1 <- table(y_test.w$pres, fit.test1)


# testing accuracy
#round(sum(diag(predtrain.t1))/sum(predtrain.t1), 3) 

# record accuracy results
fnn1<-c(round(nn_history.cv$metrics$accuracy[length(nn_history.cv$metrics$accuracy)],3),'',round(nn_results.cv[2],3) )
fnn2<-c(round(nn_history$metrics$accuracy[length(nn_history$metrics$accuracy)],3),round(nn_history$metrics$val_accuracy[length(nn_history$metrics$val_accuracy)],3),round(nn_results[2],3) )
cnn1<-c(round(cnn_history.cv$metrics$accuracy[length(cnn_history.cv$metrics$accuracy)],3),'' ,round(cnn_results.cv[2],3) )
cnn2<-c(round(cnn_history$metrics$accuracy[length(cnn_history$metrics$accuracy)],3),round(cnn_history$metrics$val_accuracy[length(cnn_history$metrics$val_accuracy)],3),round(nn_results.cv[2],3))
ct1 <- c(round(sum(diag(predtrain))/sum(predtrain), 3),'',round(sum(diag(predtrain.t))/sum(predtrain.t), 3) )
ct5 <-c(round(sum(diag(predtrain1))/sum(predtrain1), 3),'',round(sum(diag(predtrain.t1))/sum(predtrain.t1), 3) )
ac_results<- matrix(rbind(fnn1,fnn2,cnn1,cnn2,ct1,ct5),nrow = 6
)

##calculate other accuracy metrics (kappa and P-value)
#convert probabilities to ones and zeros
conv<-function(x){
  v <- matrix(0, nrow = nrow(fnn), ncol = ncol(fnn))
v[cbind(1:nrow(fnn), max.col(fnn))] <- 1
return(v)
}

fnn.con<-confusionMatrix(as.factor(max.col(fnn)),as.factor(max.col(y_test_cat)))
fnn.cv.con<-confusionMatrix(as.factor(max.col(conv(fnn.cv))),as.factor(max.col(y_test_cat)))
cnn.con<-confusionMatrix(as.factor(max.col(conv(cnn))),as.factor(max.col(y_test_cat)))
cnn.cv.con<-confusionMatrix(as.factor(max.col(conv(cnn.cv))),as.factor(max.col(y_test_cat)))
ct1.con<-confusionMatrix(fit.test,as.factor(y_test.w$pres))
ct2.con<-confusionMatrix(fit.test1,as.factor(y_test.w$pres))

a.met<- rbind(
fnn.con$overall[-c(1,3,4,5,7)],
fnn.cv.con$overall[-c(1,3,4,5,7)],
cnn.con$overall[-c(1,3,4,5,7)],
cnn.cv.con$overall[-c(1,3,4,5,7)],
ct1.con$overall[-c(1,3,4,5,7)],
ct2.con$overall[-c(1,3,4,5,7)])

acc_results<-cbind(ac_results,round(a.met,3))
# Assuming ac_results is your data frame
rownames(acc_results) <- c("FNN", "FNN.CV", "CNN", "CNN.CV", "Top100 CT", "Top500 CT")

# Create the table with kable
library(kableExtra) # Make sure you've loaded the kableExtra package
kable(acc_results, 
      col.names = c("Training Acc", "Validation Acc", "Testing Acc", "kappa", "P-Value"),
      caption = "Model accuracy results")


```

The feedforward neural network models (FNN) performed best on the training data but poorly on unseen data. The same trend is apparent with the convolutional neural network models (CNN). This trend implies that the models are over fitting the training data. The difference between the results of models which applied early stopping and cross validation (FNN.CV\|CNN.CV) and those which did not (FNN\|CNN), are not suggestively different. Which means that applying early stopping and cross validation did not improve the results of these models.

The classification models (Top100 CT\|Top500 CT) performed better than the neural network models. Though the accuracies of these models were also not satisfactory, there was great consistence between how the models performed with seen and unseen data. The classification tree built from the top 500 bag of words produced the best accuracy on unseen data.

Kappa, also known as K , is a statistic that is used to evaluate the agreement between predicted classifications and actual classifications. The kappa statistic ranges from -1 to 1:

-   A K value of 1 indicates perfect agreement beyond chance.

-   A K value of 0 indicates agreement that is exactly what would be expected by chance.

-   A K value less than 0 suggests less agreement than would be expected by chance, indicating that the models are disagreeing more than they would if their choices were random.

Kappa is particularly useful when assessing the performance of a classification model, especially in situations where class distributions are imbalanced, like in this case.

The K values for all the classification models indicate that there is a weak agreement between the predicted classifications and the actual classifications. The Top500 bag-of-words classification tree models remains the best model as it shows a higher agreement between the predicted and the actual beyond chance.

The P-value is used to determine the significance of a statistical result. A p-value of less than or equal to the chosen significance level suggests that the observation results are statistically significant. In this case the chosen significance level was 0.05, which means that all the results obtained were statistically significant. The Top500 bag-of-words classification model had a p-value on 0.051, 0.001 units above the cutoff limit. So, with some improvements such as increasing the sample size, the model has potential to produce statistically significant results

### Evaluating performance of the model with the best accuracy results

Below is a confusion matrix which shows how the top 500 bag-of-words classification tree model performed on the test dataset.

```{r ac.ct, echo=FALSE}
kable(predtrain.t1,caption="Top500 Classification tree test model confusion matrix")
```

The model perfectly classifies the content delivered by President Mbeki and President Zuma. @fig-e shows that the classification tree which was built only classifies into two classes which are related to these presidents. It is also noteworthy that these presidents have more speeches than President Mandela and President Ramaphosa. Which implies that sample size may be the reason why the model failed to successfully build and classify into the 4 expected classes.

## Conclusion

The best predictive model to use on this data is a classification tree which takes the top 500 bag-of -words from the speeches as input. A bigger and balanced sample of the president speeches is required for the model to perform well.

## References

<a name="1"> \[1\]</a> Prasanna, P.L. and Rao, D.R., 2018. Text classification using artificial neural networks. International Journal of Engineering & Technology, 7(1.1), pp.603-606.

<a name="2"> \[2\]</a> Noormanshah, W.M., Nohuddin, P.N. and Zainol, Z., 2018. Document categorization using decision tree: Preliminary study. International journal of engineering & technology, 7(4.34), pp.437-440.

<a name="2"> \[3\]</a> OpenAI. (Year). "GPT-3.5 Architecture." Available at: https://www.openai.com/gpt-3/ (Accessed: October , 2023).
